---
title: |
  | Multilevel Models  
  | (Two Different Ones)  
output:
  revealjs::revealjs_presentation:
    css: clean.css
    theme: black
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(rstanarm)

example("example_model")

fit = example_model

```



## Multilevel Models

## Cluster One

Data rarely contains a single level.

People are often nested in groups, groups are observed over years, or people are grouped by treatment levels.

> - When such levels exist, should we ignore it?

## Careful With That Axe, Euguene

```{r}
testData = data.frame(group = as.factor(rep(1:9, each = 5)), 
                      score1 = c(6, 8, 9, 12, 20, 
                                 8, 10, 11, 14, 23, 
                                 10, 12, 13, 17, 26, 
                                 12, 13, 14, 18, 26, 
                                 16, 20, 22, 23, 25,
                                 20, 22, 24, 28, 30, 
                                 23, 24, 25, 27, 32, 
                                 24, 25, 26, 28, 32, 
                                 25, 28, 29, 30, 31), 
                      score2 = c(89, 90, 94, 95, 98, 
                                 80, 81, 82, 84, 87, 
                                 74, 75, 76, 77, 78,
                                 65, 66, 67, 68, 72, 
                                 59, 60, 62, 64, 65, 
                                 50, 51, 53, 55, 58, 
                                 44, 45, 48, 49, 50, 
                                 35, 38, 39, 40, 42, 
                                 20, 22, 25, 27, 32))

library(ggplot2)

ggplot(testData, aes(score1, score2)) +
  geom_point() +
  theme_minimal()
```


##


```{r}
library(ggpmisc)

form1 = testData$score1 ~ testData$score2

ggplot(testData, aes(score1, score2)) +
  geom_point() +
  geom_smooth(method = "lm") +
  stat_poly_eq(label.x.npc = "right", formula = form1, 
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE) +
  theme_minimal()
```


## Let There Be A Little More Light

```{r}
ggplot(testData, aes(score1, score2, color = group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  ggthemes::scale_color_ptol()
```


## Wot's...Uh The Deal?

```{r}

set.seed(1001)

testData = data.frame(group = rep(1:9, each = 3), 
                      days = rep(1:3, 9), 
                      scores = sample(1:10, 27, replace = TRUE))

ggplot(testData, aes(days, scores)) +
  #geom_point() +
  geom_smooth(method = "lm")+
  theme_minimal()
```

## Louder Than Words

```{r}
ggplot(testData, aes(days, scores, group = group)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ group) +
  theme_minimal()

```


## A Saucerful Of Secrets

### What do we gain?

Better recognition of standard errors.

Exploration and estimation of group effects.

-  The opposite of a fixed-effects model.

Inference to population of groups.

## Learning To Fly

Let's assume a two-level model.

## Level 1

$$Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + e_{ij}$$
</br>

$Y_{ij}$ is the DV value for an individual at level 1.

$X_{ij}$ is the level 1 predictor.

$\beta_{0j}$ is the group intercept of the DV.

$\beta_{1j}$ is the group slope between the level 1 predictor and DV.

$e_{ij}$ is the level 1 error.

## Level 2

$$\beta_{0j} = \gamma_{00} + \gamma_{01}W_j + u_{0j}$$

$$\beta_{1j} = \gamma_{10} + u_{1j}$$

$\gamma_{00}$ is the overall intercept. 

$W_j$ is the level 2 predictor.

$\gamma_{01}$ is the overall regression coefficient between the DV and level 2 predictor.

$u_{0j}$ is the error for group deviation of the overall intercept. 

$\gamma_{10}$ is the overall regression coefficient between the DV and level 1 predictor.

$u_{ij}$ is the slope error.

## Model Types

Random intercepts

Random slopes

Random intercepts and slopes

## Random Intercepts

Intercepts are allowed to vary.

Scores for each individual are predicted by the group-varying intercept. 

Assumes fixed slopes.

## Random Slopes

Assumes fixed intercepts.

Different slopes across groups.

## Both

Slopes and intercepts are allowed to vary over groups.

## Extensions

Naturally, there are many extensions to multilevel models.

- SEM

- GLM distribution families and link functions

## Obscured By Clouds

Perhaps the most important extension is the nonlinear mixed effect model.

As opposed to *linearization*, nonlinear models actually model the process that created the response.

This leads to better prediction.

## Take Up Thy Stethoscope And Walk

SAS -- *GLIMMIX*

Stata -- *me*

SPSS -- *MIXED*

*Matlab* -- *nlmefit*

Python -- *PyMC3*

R -- *lme4*, *lmer*, *nlme*

*Julia* -- *MixedModels*


## Bayesian Data Analysis

##

![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif)

##

![](https://upload.wikimedia.org/wikipedia/commons/b/bf/Pierre-Simon%2C_marquis_de_Laplace_%281745-1827%29_-_Gu%C3%A9rin.jpg)

## What Is It?

We have all likely seen Bayes' Theorem.

$$ P(A|B) = \frac{P(B|A)P|A}{P|B} $$

There are a great many classical examples, but we are going to do more with this formula.


## Your Possible Pasts

1. We have a feeling about the way our world works. We might even have some supporting data. This is our *prior*. We can specify a lot within our prior (distribution shape and properties)

2. After running our model, we can *update* what we know about our parameters. 

3. We can then create a *posterior probability distribution*.

Note: At its peak, this is a cycle.


## Round And Around

![](scientific_method_770.jpg)

## Us And Them

There is a brewing war between the Frequentists and the Bayesians.

Only you can decide which side you will be on.

## Probability

> - Frequentist 

I am going to assume that my parameter is zero. What is the probability that my observed parameter is a certain magnitude different than zero?

> - Bayesian

What is the probability that my parameter is not zero?

## Interval Estimates

> - Frequentist

I will conduct my analyses an infinitie number of times and calculate an interval each time; a certain percentage of intervals will contain the true value. I will now show you one of those intervals.

> - Bayesian

The probability that the true value falls in this interval is *P*.

## Goodbye Blue Sky

The *p*-value that we all know and love is of no consequence to the Bayesian. 

## Welcome To The Machine

*Model criticism* is an important part of BDA.

## 

```{r, fig.height=5}
plot(fit)
```


##

```{r}
pp_check(fit)
```

##

```{r}
pp_check(fit, plotfun = "boxplot")
```

##

```{r, echo = FALSE, message = FALSE, include = FALSE}
test = posterior_vs_prior(fit)
```

```{r}
test + theme(legend.position = "none") + coord_flip()
```


##

```{r, echo = FALSE, message = FALSE, include = FALSE}
test = posterior_vs_prior(fit, group_by_parameter = TRUE)
```

```{r}
test + theme(legend.position = "none") + coord_flip()
```


## Two Suns In The Sunset

BDA can be applied to **anything**.

*t*-tests

Any regression

SEM

And...

## Multilevel Bayesian

```{r}
herdData = lme4::cbpp

form1 = herdData$incidence ~ herdData$size

ggplot(herdData, aes(incidence, size)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_minimal() +
  stat_poly_eq(label.x.npc = "left", formula = form1, 
                aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
                parse = TRUE)
```


##

```{r}
ggplot(herdData, aes(incidence, size)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~ herd) +
  theme_minimal()
```


## Bayesian Software

WinBUGS

JAGS

Stan (RStan, PyStan, Matlab)

## Absolutely Curtains

## What Do You Want From Me?
Mobile Data Collection

Generalized Additive Models

Using The CRC

Markdown Magic

Making The Most Of Text

Fixed, Random, and Mixed Effects Models

Version Control And Collaboration With Github

Julia